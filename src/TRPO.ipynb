{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let us create our inputs\n",
    "import gym \n",
    "from nn import PolicyNN, ValueNN\n",
    "\n",
    "# lets get our environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Policy parameters (NN) = \"theta\". 2 hidden layers w/ 32 nodes. softmax output for each action\n",
    "theta = PolicyNN(\n",
    "    env.observation_space.shape[0],\n",
    "    2,\n",
    "    32,\n",
    "    env.action_space.n\n",
    "    )\n",
    "\n",
    "# Value function parameters (NN) = \"V\". outputs value of state (scalar)\n",
    "V = ValueNN(\n",
    "    env.observation_space.shape[0],\n",
    "    2,\n",
    "    32,\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1, generate trajectories\n",
    "'''\n",
    "As input, we will need:\n",
    "    - the gym environment\n",
    "    - a policy \"pi\" which takes in the state and outputs an action\n",
    "As output, we return:\n",
    "    - the trajectory of (s, a, r) tuples\n",
    "'''\n",
    "def generate_trajectory(env, pi, render_last_step: bool=False ):\n",
    "    episode = []\n",
    "    s = env.reset()\n",
    "    a = pi(s)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        s_p, r, done, info = env.step(a)\n",
    "        episode.append((s, a, r))\n",
    "        s = s_p\n",
    "        a = pi(s_p)\n",
    "        if done:\n",
    "            env.render()\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2, estimate the advantage of the trajectory\n",
    "'''\n",
    "Note: Advantage = A(a, s) = sum(r_t + gamma^(t+1) * V(s_t+1) - V(s_t)) where t = 0,1,2,...,T-1, and T = len(trajectory)\n",
    "As input, we will need:\n",
    "    - the trajectory, list of (s, a, r) tuples\n",
    "    - gamma (discount)\n",
    "    - value function V which takes in the state and outputs a scalar value\n",
    "As output, we return:\n",
    "    - a list of scalars which is the discounted advantage at every step \n",
    "'''\n",
    "from typing import List\n",
    "def advantage_estimates(trajectory, gamma : float, V) -> List[float]:\n",
    "    advantages = {}\n",
    "    for t in range(0, len(trajectory)-1):\n",
    "        s_t, a_t, r_t = trajectory[t]\n",
    "        s_t_1, _, _ = trajectory[t+1]\n",
    "        advantages[(s_t, a_t)] = (r_t + gamma * V(s_t_1) - V(s_t))\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3, sample estimates for objective L_theta_k and KL-Divergence constraint H using advantage estimates\n",
    "'''\n",
    "Single path sample estimates for L_theta_k and H\n",
    "Equations:\n",
    "    - g_k = gradient_theta(L_theta_k(theta))\n",
    "    - L_theta_k = sum(discounted future rewards -> just use advantage), where s_0~p_0, a~q, q(a|s) = policy_theta_k(a|s)\n",
    "    - H = FIM = sum(KL_divergence(pi_theta_old(*|s_n) || pi_theta(*|s_n)))\n",
    "Inputs:\n",
    "    - policy_theta_k\n",
    "    - DONT need action distribution (for single path, this will be equivalent to our policy)\n",
    "    - number of samples\n",
    "    - to pass along\n",
    "        - env (calling generate trajectory)\n",
    "        - V (calling advantage estimate)\n",
    "Outputs:\n",
    "    - estimated objective L_theta_k(theta)\n",
    "    - estimated constraint KL divergence H \n",
    "'''\n",
    "\n",
    "def single_path_sample_estimator(pi : PolicyNN, num_samples: int):\n",
    "    # set of trajectories D_k\n",
    "    D_k = []\n",
    "    for i in range(num_samples):\n",
    "        D_k.append(generate_trajectory(env, p))\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfc5e705b253edb9a4db6e444ecc815b4e3b76e07f807b8daf8ca08e934c3f1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
