{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1, generate trajectories\n",
    "'''\n",
    "As input, we will need:\n",
    "    - the gym environment\n",
    "    - a policy \"pi\" which takes in the state and outputs an action\n",
    "As output, we return:\n",
    "    - the trajectory of (s, a, r, a_prob) tuples\n",
    "'''\n",
    "import numpy\n",
    "def generate_trajectory(env, pi, render_last_step: bool=False ):\n",
    "    episode = []\n",
    "    s = env.reset()\n",
    "    a, a_prob = pi(torch.tensor(s))\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        s_p, r, done, info = env.step(torch.argmax(a)[0])\n",
    "        episode.append((s, a, r, a_prob))\n",
    "        s = s_p\n",
    "        a, a_prob = pi(torch.tensor(s_p))\n",
    "        if done:\n",
    "            env.render()\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2, estimate the advantage of the trajectory\n",
    "'''\n",
    "Note: Advantage = A(a, s) = sum(r_t + gamma^(t+1) * V(s_t+1) - V(s_t)) where t = 0,1,2,...,T-1, and T = len(trajectory)\n",
    "As input, we will need:\n",
    "    - the trajectory, list of (s, a, r) tuples\n",
    "    - gamma (discount)\n",
    "    - value function V which takes in the state and outputs a scalar value\n",
    "As output, we return:\n",
    "    - advantage estimate per time step\n",
    "'''\n",
    "from typing import List\n",
    "def advantage_estimates(trajectory, gamma : float, V) -> List[float]:\n",
    "    advantages = []\n",
    "    for t in range(0, len(trajectory)-1):\n",
    "        s_t, a_t, r_t, _ = trajectory[t]\n",
    "        s_t_1, _, _, _ = trajectory[t+1]\n",
    "        advantages.append(r_t + gamma * V(s_t_1) - V(s_t))\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3, sample estimates for objective L_theta_k and KL-Divergence constraint H using advantage estimates\n",
    "'''\n",
    "Single path sample estimates for L_theta_k and H\n",
    "Equations:\n",
    "    - g_k = gradient_theta(L_theta_k(theta))\n",
    "    - L_theta_k = sum(discounted future rewards -> just use advantage), where s_0~p_0, a~q, q(a|s) = policy_theta_k(a|s)\n",
    "    - H = FIM = sum(KL_divergence(pi_theta_old(*|s_n) || pi_theta(*|s_n)))\n",
    "Inputs:\n",
    "    - policy_theta_k\n",
    "    - DONT need action distribution (for single path, this will be equivalent to our policy)\n",
    "    - number of samples\n",
    "    - to pass along\n",
    "        - env (calling generate trajectory)\n",
    "        - V (calling advantage estimate)\n",
    "        - gamma (calling advantage estimate)\n",
    "Outputs:\n",
    "    - estimated objective L_theta_k(theta)\n",
    "    - estimated constraint KL divergence H \n",
    "'''\n",
    "import torch\n",
    "def single_path_sample_estimator(pi, env, V, gamma, num_samples: int):\n",
    "    # set of trajectories D_k\n",
    "    D_k = []\n",
    "    # set of Advantages\n",
    "    A_k = []\n",
    "    for i in range(num_samples):\n",
    "        trajectory = generate_trajectory(env, pi)\n",
    "        D_k.append(trajectory)\n",
    "        A_k.append(advantage_estimates(trajectory, gamma))\n",
    "    # estimate policy gradient\n",
    "    g_k = None\n",
    "    for traj_idx in range(len(D_k)):\n",
    "        episode = D_k[traj_idx]\n",
    "        adv = A_k[traj_idx]\n",
    "        for t in range(len(episode)):\n",
    "            s, a, r, a_prob = episode[t]\n",
    "            if g_k ==None:\n",
    "                g_k = torch.zeros(a_prob.grad.shape)\n",
    "            g_k += a_prob.grad * torch.tensor(adv[t])\n",
    "           \n",
    "    return g_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/nn.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  action_probs = F.softmax(actions)    # linear output\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor(0) (<class 'torch.Tensor'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000005?line=26'>27</a>\u001b[0m NUM_SAMPLES \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000005?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000005?line=28'>29</a>\u001b[0m     \u001b[39mprint\u001b[39m(single_path_sample_estimator(theta, env, V, GAMMA, NUM_SAMPLES))\n",
      "\u001b[1;32m/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb Cell 3'\u001b[0m in \u001b[0;36msingle_path_sample_estimator\u001b[0;34m(pi, env, V, gamma, num_samples)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000003?line=24'>25</a>\u001b[0m A_k \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000003?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_samples):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000003?line=26'>27</a>\u001b[0m     trajectory \u001b[39m=\u001b[39m generate_trajectory(env, pi)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000003?line=27'>28</a>\u001b[0m     D_k\u001b[39m.\u001b[39mappend(trajectory)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000003?line=28'>29</a>\u001b[0m     A_k\u001b[39m.\u001b[39mappend(advantage_estimates(trajectory, gamma))\n",
      "\u001b[1;32m/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb Cell 1'\u001b[0m in \u001b[0;36mgenerate_trajectory\u001b[0;34m(env, pi, render_last_step)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000001?line=13'>14</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000001?line=15'>16</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000001?line=16'>17</a>\u001b[0m     s_p, r, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(torch\u001b[39m.\u001b[39;49margmax(a))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000001?line=17'>18</a>\u001b[0m     episode\u001b[39m.\u001b[39mappend((s, a, r, a_prob))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joey/Documents/UT/Sp22/RL_Final_Project/src/TRPO.ipynb#ch0000001?line=18'>19</a>\u001b[0m     s \u001b[39m=\u001b[39m s_p\n",
      "File \u001b[0;32m~/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=16'>17</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=17'>18</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=11'>12</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=12'>13</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:118\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=115'>116</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=116'>117</a>\u001b[0m     err_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=117'>118</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(action), err_msg\n\u001b[1;32m    <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=118'>119</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCall reset before using step method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/joey/Documents/UT/Sp22/RL_Final_Project/myenv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=119'>120</a>\u001b[0m     x, x_dot, theta, theta_dot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor(0) (<class 'torch.Tensor'>) invalid"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Runner\n",
    "'''\n",
    "# first, let us create our inputs\n",
    "import gym \n",
    "from nn import PolicyNN, ValueNN\n",
    "\n",
    "# lets get our environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Policy parameters (NN) = \"theta\". 2 hidden layers w/ 32 nodes. softmax output for each action\n",
    "theta = PolicyNN(\n",
    "    env.observation_space.shape[0],\n",
    "    2,\n",
    "    32,\n",
    "    env.action_space.n\n",
    "    )\n",
    "\n",
    "# Value function parameters (NN) = \"V\". outputs value of state (scalar)\n",
    "V = ValueNN(\n",
    "    env.observation_space.shape[0],\n",
    "    2,\n",
    "    32,\n",
    "    1\n",
    ")\n",
    "GAMMA = 1\n",
    "NUM_SAMPLES = 1\n",
    "for _ in range(1):\n",
    "    print(single_path_sample_estimator(theta, env, V, GAMMA, NUM_SAMPLES))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfc5e705b253edb9a4db6e444ecc815b4e3b76e07f807b8daf8ca08e934c3f1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
